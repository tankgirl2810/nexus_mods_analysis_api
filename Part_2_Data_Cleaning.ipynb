{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Cleaning\n",
    "\n",
    "This notebook contains the code for cleaning and preprocessing the NexusMods data.\n",
    "\n",
    "## Overview\n",
    "1. **Description Translation** - Detect language and translate non-English mod descriptions\n",
    "2. **Patreon URL Extraction** - Extract Patreon URLs from descriptions\n",
    "3. **Category Normalization** - Use fuzzy matching to normalize category names\n",
    "\n",
    "## Output\n",
    "- `TranslatedModData` table with cleaned descriptions (didn't end up using this)\n",
    "- `GameCategories` table with normalized category groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy connection setup\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://username:password@server.database.windows.net/NexusModsDB?driver=ODBC+Driver+17+for+SQL+Server&Connect Timeout=60\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing configuration\n",
    "CHUNK_SIZE = 500\n",
    "BATCH_SIZE = 500\n",
    "SAVE_FILE = \"processed_data.parquet\"\n",
    "CHECKPOINT_FILE = \"checkpoint.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Description Translation & Patreon URL Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATREON_REGEX = re.compile(r\"https?://(?:www\\.)?patreon\\.com/[^\\s\\]]+\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_detect_translate(text):\n",
    "    '''\n",
    "    Clean text, detect language, translate if needed, and extract Patreon URL.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (detected_language, translated_text, patreon_url)\n",
    "    '''\n",
    "    try:\n",
    "        # Extract Patreon URL\n",
    "        patreon_match = PATREON_REGEX.findall(text)\n",
    "        patreon_url = patreon_match[0].split(\"]\")[0] if patreon_match else None\n",
    "        \n",
    "        # Remove BBCode and URLs\n",
    "        text_no_bbcode = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "        text_no_bbcode = re.sub(r\"https?://\\S+\", \"\", text_no_bbcode)\n",
    "        \n",
    "        # Parse HTML and get clean text\n",
    "        clean_text = BeautifulSoup(text_no_bbcode, \"lxml\").get_text(separator=\" \")\n",
    "        clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
    "        \n",
    "        # Detect language\n",
    "        lang = detect(clean_text)\n",
    "        \n",
    "        # Translate if not English\n",
    "        if lang != \"en\":\n",
    "            translated_text = GoogleTranslator(source=lang, target=\"en\").translate(clean_text)\n",
    "        else:\n",
    "            translated_text = clean_text\n",
    "        \n",
    "        return lang, translated_text, patreon_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, text, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mods_for_translation():\n",
    "    '''Load mod descriptions that haven't been translated yet.'''\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        c.game_id,\n",
    "        c.domain_name, \n",
    "        c.mod_id, \n",
    "        c.[description], \n",
    "        t.[detected_language]\n",
    "    FROM [dbo].[CleanedModData] as c \n",
    "    LEFT JOIN [dbo].[TranslatedModData] as t \n",
    "        ON t.game_id = c.game_id AND t.mod_id = c.mod_id \n",
    "    WHERE c.description IS NOT NULL \n",
    "        AND t.[detected_language] IS NULL\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_translations():\n",
    "    '''Process all mod descriptions - detect language, translate, extract Patreon URLs.'''\n",
    "    df = load_mods_for_translation()\n",
    "    print(f\"Loaded {len(df)} mods for translation.\")\n",
    "    \n",
    "    # Load existing processed data if available\n",
    "    if os.path.exists(SAVE_FILE):\n",
    "        df_processed = pd.read_parquet(SAVE_FILE)\n",
    "        processed_ids = set(df_processed.index)\n",
    "    else:\n",
    "        df_processed = pd.DataFrame(columns=['detected_language', 'translated_description', 'patreon_url'])\n",
    "        processed_ids = set()\n",
    "    \n",
    "    # Load checkpoint\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        last_index = joblib.load(CHECKPOINT_FILE)\n",
    "    else:\n",
    "        last_index = 0\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Process in chunks\n",
    "    for i in tqdm(range(last_index, total_rows, CHUNK_SIZE), desc=\"Processing chunks\"):\n",
    "        chunk = df.iloc[i:i + CHUNK_SIZE]\n",
    "        chunk = chunk[~chunk.index.isin(processed_ids)]\n",
    "        \n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        \n",
    "        # Process each row\n",
    "        results = chunk['description'].apply(lambda x: pd.Series(clean_detect_translate(x)))\n",
    "        chunk[['detected_language', 'translated_description', 'patreon_url']] = results\n",
    "        final_chunk = chunk[['game_id', 'mod_id', 'detected_language', 'translated_description', 'patreon_url']]\n",
    "        \n",
    "        # Append to processed data\n",
    "        df_processed = pd.concat([df_processed, final_chunk])\n",
    "        df_processed.to_parquet(SAVE_FILE, index=True, engine=\"pyarrow\", allow_truncated_timestamps=True)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        joblib.dump(i + CHUNK_SIZE, CHECKPOINT_FILE)\n",
    "    \n",
    "    print(f\"Processing complete. Data saved to: {SAVE_FILE}\")\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_translations_to_sql():\n",
    "    '''Upload processed translations to SQL database.'''\n",
    "    df_processed = pd.read_parquet(SAVE_FILE)\n",
    "    df = df_processed[[\"game_id\", \"mod_id\", \"detected_language\", \"translated_description\", \"patreon_url\"]]\n",
    "    df = df.dropna(subset=[\"translated_description\"])\n",
    "    df[\"detected_language\"] = df[\"detected_language\"].fillna(\"\")\n",
    "    df[\"patreon_url\"] = df[\"patreon_url\"].fillna(\"\")\n",
    "    \n",
    "    merge_sql = \"\"\"\n",
    "        MERGE INTO TranslatedModData AS target\n",
    "        USING (SELECT :game_id AS game_id, :mod_id AS mod_id, :lang AS detected_language, \n",
    "                      :desc AS translated_description, :patreon AS patreon_url) AS source\n",
    "        ON target.game_id = source.game_id AND target.mod_id = source.mod_id\n",
    "        WHEN MATCHED AND target.translated_description IS NULL THEN \n",
    "            UPDATE SET detected_language = source.detected_language, \n",
    "                       translated_description = source.translated_description, \n",
    "                       patreon_url = source.patreon_url\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (game_id, mod_id, detected_language, translated_description, patreon_url)\n",
    "            VALUES (source.game_id, source.mod_id, source.detected_language, \n",
    "                    source.translated_description, source.patreon_url);\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Inserting batches\"):\n",
    "            batch_df = df.iloc[i:i + BATCH_SIZE]\n",
    "            batch_data = batch_df.to_dict(orient=\"records\")\n",
    "            \n",
    "            try:\n",
    "                for row in batch_data:\n",
    "                    conn.execute(text(merge_sql), {\n",
    "                        \"game_id\": row[\"game_id\"],\n",
    "                        \"mod_id\": row[\"mod_id\"],\n",
    "                        \"lang\": row[\"detected_language\"],\n",
    "                        \"desc\": row[\"translated_description\"],\n",
    "                        \"patreon\": row[\"patreon_url\"]\n",
    "                    })\n",
    "                conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting batch: {e}\")\n",
    "    \n",
    "    print(\"TranslatedModData table populated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Category Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_categories():\n",
    "    '''Load category data from database.'''\n",
    "    query = \"\"\"\n",
    "    SELECT [game_id], [game_name], [domain_name], [category_id], [category_name],\n",
    "           [total_mods], [total_endorsements], [total_unique_downloads],\n",
    "           [group_category], [group_id]\n",
    "    FROM [dbo].[GameCategories]\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_category(name):\n",
    "    '''\n",
    "    Preprocess category name for normalization.\n",
    "    - Lowercase and strip\n",
    "    - Remove common words (and, &, mod, mods)\n",
    "    - Apply standard replacements\n",
    "    - Group similar categories\n",
    "    '''\n",
    "    if not name or name.strip() == \"---\":\n",
    "        return \"---\"\n",
    "    \n",
    "    name = name.lower().strip()\n",
    "    name = re.sub(r'\\b(and|&)\\b', '', name)\n",
    "    name = re.sub(r'\\bmod(s)?\\b', '', name)\n",
    "    \n",
    "    # Standard replacements\n",
    "    replacements = {\n",
    "        \"armor\": \"armour\",\n",
    "        \"user interface\": \"ui\",\n",
    "        \"abilities\": \"ability\",\n",
    "        \"animations\": \"animation\",\n",
    "        \"weapons\": \"weapon\",\n",
    "        \"items\": \"item\",\n",
    "        \"skins\": \"skin\",\n",
    "        \"sounds\": \"sound\",\n",
    "        \"maps\": \"map\",\n",
    "        \"levels\": \"level\",\n",
    "        \"vehicles\": \"vehicle\",\n",
    "        \"textures and meshes\": \"textures\",\n",
    "        \"texture pack\": \"textures\",\n",
    "        \"textures\": \"texture\",\n",
    "        \"retextures\": \"texture\",\n",
    "        \"hud\": \"ui\",\n",
    "        \"audio\": \"sound\",\n",
    "        \"music\": \"sound\",\n",
    "        \"sfx\": \"sound\",\n",
    "        \"voice\": \"sound\",\n",
    "        \"visuals\": \"visual\",\n",
    "        \"graphics\": \"graphic\",\n",
    "        \"miscellaneous\": \"misc\",\n",
    "        \"miscallenous\": \"misc\",\n",
    "        \"miscellanneous\": \"misc\"\n",
    "    }\n",
    "    \n",
    "    # Category groups for normalization\n",
    "    category_groups = {\n",
    "        \"vehicle\": [\"vehicle\", \"vehicle - aeroplanes\", \"vehicle - aircraft\", \"vehicle - boat\", \n",
    "                    \"vehicle - buses\", \"vehicle - land\", \"vehicle - ship\", \"vehicle - train\", \"vehicle - truck\"],\n",
    "        \"sound\": [\"sound\", \"sound - misc\", \"sound - music\", \"sound - sfx\", \"sound - voice\", \"sound pack\"],\n",
    "        \"misc\": [\"misc\", \"misc item\", \"misc tool\", \"miscallenous\"],\n",
    "        \"map\": [\"map\", \"map - adventure\", \"map - campaign\", \"map - multiplayer\", \"map - singleplayer\"],\n",
    "        \"gameplay\": [\"gameplay\", \"gameplay changes\", \"gameplay effect\", \"gameplay mechanic\", \"gameplay tweak\"],\n",
    "        \"cosmetic\": [\"clothes\", \"clothing\", \"hair\", \"jewelry\", \"jewellery\", \"apparel\", \"outfits\", \"fashion\"],\n",
    "        \"item\": [\"item\", \"item - food/drinks/chems/etc\", \"item - misc\", \"item pack\"],\n",
    "        \"magic\": [\"mage\", \"magic\", \"magic - alchemy\", \"magic - gameplay\", \"magic - spell & enchantment\"]\n",
    "    }\n",
    "    \n",
    "    # Apply replacements\n",
    "    for key, value in replacements.items():\n",
    "        name = re.sub(rf'\\b{re.escape(key)}\\b', value, name)\n",
    "    \n",
    "    # Apply category groups\n",
    "    for category, variations in category_groups.items():\n",
    "        if any(variation in name for variation in variations):\n",
    "            name = category\n",
    "            break\n",
    "    \n",
    "    # Normalize modding tools\n",
    "    if re.search(r'\\b(tool|modding tool|modding resources)\\b', name):\n",
    "        name = \"modding tool\"\n",
    "    \n",
    "    # Remove trailing 's' from words (simple pluralization)\n",
    "    words = name.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word.endswith('s') and not word.endswith(('ss', 'us', 'is', 'ous', 'ies', 'es')):\n",
    "            word = word[:-1]\n",
    "        processed_words.append(word)\n",
    "    \n",
    "    return ' '.join(processed_words).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_categories():\n",
    "    '''Main function to normalize all categories using fuzzy matching.'''\n",
    "    df = load_categories()\n",
    "    print(f\"Loaded {len(df)} categories.\")\n",
    "    \n",
    "    # Preprocess category names\n",
    "    df['clean_category'] = df['group_category'].astype(str).apply(preprocess_category)\n",
    "    \n",
    "    # Extract unique categories for fuzzy matching\n",
    "    categories = df[['group_id', 'clean_category']].astype(str).values.tolist()\n",
    "    \n",
    "    # Apply fuzzy matching\n",
    "    category_mapping = {}\n",
    "    for cat_id, cat_name in tqdm(categories, desc=\"Fuzzy matching categories\"):\n",
    "        matches = process.extract(cat_name, [c[1] for c in categories], limit=5, scorer=fuzz.ratio)\n",
    "        best_match = next((m[0] for m in matches if m[1] > 80 and m[0] != cat_name), cat_name)\n",
    "        category_mapping[cat_id] = best_match\n",
    "    \n",
    "    # Create mapping dataframe\n",
    "    mapping_df = pd.DataFrame(category_mapping.items(), columns=['group_id', 'new_group_category'])\n",
    "    mapping_df['new_group_id'] = mapping_df.groupby('new_group_category').ngroup()\n",
    "    \n",
    "    # Merge back\n",
    "    df['group_id'] = df['group_id'].astype(int)\n",
    "    mapping_df['group_id'] = mapping_df['group_id'].astype(int)\n",
    "    df = df.merge(mapping_df[['group_id', 'new_group_category', 'new_group_id']], on='group_id', how='left')\n",
    "    \n",
    "    print(f\"Normalized {len(df)} categories into {mapping_df['new_group_id'].nunique()} groups.\")\n",
    "    \n",
    "    return df, mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_categories_in_sql(df):\n",
    "    '''Update normalized categories in SQL database.'''\n",
    "    df_dedup = df.sort_values(by='group_id').drop_duplicates(subset=['group_id'], keep='last')\n",
    "    \n",
    "    # Build merge query\n",
    "    values = ', '.join(\n",
    "        f\"({row.group_id}, '{row.new_group_category}', {row.new_group_id})\" \n",
    "        for _, row in df_dedup.iterrows()\n",
    "    )\n",
    "    \n",
    "    merge_query = text(f\"\"\"\n",
    "    WITH DeduplicatedSource AS (\n",
    "        SELECT DISTINCT group_id, new_group_category, new_group_id\n",
    "        FROM (VALUES {values}) AS tmp (group_id, new_group_category, new_group_id)\n",
    "    )\n",
    "    MERGE INTO [dbo].[GameCategories] AS gc\n",
    "    USING DeduplicatedSource AS tmp\n",
    "    ON gc.group_id = tmp.group_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET \n",
    "            gc.new_group_category = tmp.new_group_category,\n",
    "            gc.new_group_id = tmp.new_group_id;\n",
    "    \"\"\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(merge_query)\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"Categories updated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
